== Scaling

image::scaling-banner.jpg[]

In this chapter, we'll explore how Bitcoin scales, and doesn't
scale. We'll look a bit into the past to see how people have reasoned
about scaling in the past. The bulk of this chapter explains various
approaches to scaling Bitcoin. We'll talk about vertical, horizontal,
inward, and layered scaling and discuss whether they do or don't
interfere with Bitcoin's value proposition.

Different people ascribe different definitions to the word scaling, such as
increasing the transaction capacity of the blockchain, using the
blockchain more efficiently, or developing systems on top of Bitcoin.

In the context of Bitcoin, and of this book, we will define scaling as
_increasing Bitcoin's usage capacity without compromising its
censorship resistance_. This definition would encompass several
different kinds of changes, for example:

* Making transaction inputs use fewer bytes
* Improving signature verification performance
* Make the peer-to-peer network use less bandwidth
* Transaction batching
* Layered architecture

We'll soon dive into different approaches for scaling, but let's start
with a brief overview of Bitcoin's history within the context of scaling.

=== History

Scaling has been a focal point of discussion since the genesis of Bitcoin. The
very first sentence of the
https://www.metzdowd.com/pipermail/cryptography/2008-November/014814.html[very
first mail response] to Satoshi's announcement of the bitcoin white paper on the
Cryptography mailing list was about scaling:

[quote, James A. Donald and Satoshi Nakamoto, Cryptography email list ]
____
Satoshi Nakamoto wrote: +
> I've been working on a new electronic cash system that's fully +
> peer-to-peer, with no trusted third party. +
> +
> The paper is available at: +
> http://www.bitcoin.org/bitcoin.pdf

We very, very much need such a system, but the way I understand your
proposal, it does not seem to scale to the required size.
____

The conversation itself might not be very interesting or accurate, but
it shows that scaling has been a concern from the very beginning.

Scaling discussions reached peak interest around 2015-2017, when many
different ideas floated around about if and how to increase the
maximum block size limit. This is a rather uninteresting discussion
about changing a parameter in the source code, a change that doesn't
fundamentally solve anything, but pushes the problem of scaling
further into the future, building technical debt.

In 2015, a conference called https://scalingbitcoin.org/[Scaling
Bitcoin] was held in Montreal, with a follow-up conference six months
later in Hong Kong and thereafter in a number of other locations to
address scaling. Many Bitcoin developers and others gathered there to
discuss various scaling issues and proposals. Most of these
discussions didn't revolve around block size increases but more long-term
solutions.

After the Hong Kong conference in December 2015, Gregory Maxwell
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011865.html[summarized
his view] on many of the discussions and started off with
some general scaling philosophy.

[quote, Gregory Maxwell on Bitcoin-dev mailing list, Capacity increases for the Bitcoin system.]
____
With the available technology, there are fundamental trade-offs
between scale and decentralization. If the system is too costly people
will be forced to trust third parties rather than independently
enforcing the system's rules. If the Bitcoin blockchain’s resource
usage, relative to the available technology, is too great, Bitcoin
loses its competitive advantages compared to legacy systems because
validation will be too costly (pricing out many users), forcing trust
back into the system.  If capacity is too low and our methods of
transacting too inefficient, access to the chain for dispute
resolution will be too costly, again pushing trust back into the
system.
____

He speaks about the trade-off between throughput and
decentralization. If you allow for bigger blocks, you will push some
people off the network because they don't have the resources to verify
the blocks anymore. But on the other hand, if access to block space
becomes more expensive, fewer people will afford to use it as a
dispute resolution mechanism. In both these cases, users are pushed
towards trusted services.

He further summarizes the many approaches to scaling presented at the
conference. Among them are more computationally efficient signature
verifications, _segregated witness_ including a block size limit
change, a more space-efficient block propagation mechanism, and
building protocols on top of Bitcoin in layers. Many of these
approaches have since been implemented.

=== Scaling approaches

As hinted above, scaling Bitcoin doesn't necessarily have to be about
increasing the block size limit or other limits. We'll go through some
general scaling approaches, of which some don't suffer from the
throughput-decentralization trade-off mentioned in the previous
section.

==== Vertical scaling

Vertical scaling is the process of increasing the computing resources
of the machines processing data. With Bitcoin, this would be
the full node, the machine that verifies the blockchain on behalf of
its user.

Typically, vertical scaling in Bitcoin would be an increase of the
block size limit. This would require some full nodes to upgrade their
hardware to keep up with the increasing computational
demands. The downside is that it happens at the cost of
decentralization, as was discussed in the previous section and more
in-depth in <<_full_node_decentralization>>.

Besides the negative effects on full node decentralization, vertical
scaling also potentially negatively impacts Bitcoin's
<<_miner_decentralization,mining decentralization>> and security in
non-obvious ways. Let's have a look at how miners "`should`"
operate. Say a miner mines a block at height 7 and publishes that
block to the Bitcoin network. It will take some time for this block to
reach broad acceptance mainly due to two factors:

* Transfer of the block between peers takes time due to bandwidth
  limitations.
* Verification of the block takes time.

During the time it takes for block 7 to propagate, many miners still
mine on top of block 6 because they haven't received and validated
block 7 yet. During this time, if any of these miners find a new
block at height 7, we're going to have two competing blocks at that height.
There can only be one block at height 7, which means one of the two candidates
must become stale.

In short, stale blocks happen because it takes time for blocks to
propagate, and the more time propagation takes, the higher the
probability of stale blocks.

Suppose that the block size limit is lifted and that the average block
size increases substantially. Blocks would then propagate slower across the
network due to bandwidth limitations and verification time. An increased time
for propagation will also increase the chance of stale blocks.

Miners don't like to have their blocks staled because they'll lose
their block reward, so they will do what they can to avoid this
scenario. Among the things they can do are:

* Postpone validation of an incoming block, also known as
  <<bip66splits,_validationless mining_>>. Just check the block
  header's proof-of-work and mine on top of it while you download the
  full block and validate it.
* Connect to a mining pool with greater bandwidth and connectivity.

Validationless mining further undermines the
<<_full_node_decentralization, full node decentralization>> of the
system since the miner, at least temporarily, defers to trusting
incoming blocks. It also hurts security to some degree because a
portion of the network's computing power is potentially building on an
invalid blockchain, instead of building on the strongest valid chain.

The second bullet point has a negative effect on
<<_miner_decentralization,miner decentralization>>, because the larger
pools usually are the ones with the best network connectivity and
bandwidth.

==== Horizontal scaling

Horizontal scaling refers to techniques that divide the workload
across multiple machines. While this is a prevalent scaling approach
among popular websites and databases, it's not easily done in
Bitcoin.

Many people refer to this Bitcoin scaling approach as _sharding_. You
let each full node verify just a part of the blockchain. Peter Todd
has put a lot of thought into the concept of sharding. He
https://petertodd.org/2015/why-scaling-bitcoin-with-sharding-is-very-hard[wrote
a blog post] explaining sharding from a high level, and also presented
his own idea called _treechains_. The article is a difficult read,
but he makes some general points that are more digestible.

[quote, Peter Todd on his blog, Why Scaling Bitcoin With Sharding Is Very Hard]
____
In sharded systems the “full node defense” doesn’t work, at least
directly. The whole point is that not everyone has all the data, so
you have to decide what happens when it’s not available.
____

Then he explains various ideas on how to tackle sharding, or
horizontal scaling. Towards the end he concludes:

[quote, Peter Todd on his blog, Why Scaling Bitcoin With Sharding Is Very Hard]
____
There’s a big problem though: holy !@#$ is the above complex compared
to Bitcoin! Even the “kiddy” version of sharding - my linearization
scheme rather than zk-SNARKS - is probably one or two orders of
magnitude more complex than using the Bitcoin protocol is right now,
yet right now a huge % of the companies in this space seem to have
thrown their hands up and used centralized API providers
instead. Actually implementing the above and getting it into the hands
of end-users won’t be easy.

On the other hand, decentralization isn’t cheap: using PayPal is one
or two orders of magnitude simpler than the Bitcoin protocol.
____

The conclusion he makes is that sharding _might_ be technically
possible, but it comes at the cost of tremendous complexity. Given
that many users already find Bitcoin too complex and instead use
centralized services, it's going to be hard to convince them to use
something even more complex.

==== Inward scaling

While horizontal and vertical scaling has worked out well historically
in centralized systems like databases and web servers, they don't seem
to be suitable for a decentralized network like Bitcoin due to their
centralizing effects.

An approach that gets far too little appreciation is what we can call
_inward scaling_, which translates to "do more with less". It refers
to the constantly ongoing work by many developers to optimize the
algorithms already in place so that we can do more within the existing
limits of the system.

The amount of improvement that's been done through inward scaling is
impressive, to say the least. To give you a high-level view of the
improvements over the years, Jameson Lopp
https://blog.lopp.net/bitcoin-core-performance-evolution/[has run
benchmark tests] on blockchain synchronization, comparing many
different versions of Bitcoin Core going back to version 0.8.

.Initial block download performance of various versions of Bitcoin Core. On the Y-axis is the block height synced and on the X-axis is the time it took to sync to that height. Source: https://blog.lopp.net/bitcoin-core-performance-evolution/
image::Bitcoin-Core-Sync-Performance-1.png[{big-width}]

The different lines represent different versions of Bitcoin Core. The
leftmost line is the latest one used in this test; version 22.0,
released in September 2021, which took 396 minutes to fully sync. The
rightmost one is version 0.8 from November 2013, which took 3452
minutes. All of this, roughly 10x, improvement is due to inward
scaling.

The improvements could be categorized as either space (RAM, disk,
bandwidth, etc.) savings or computational savings. Both categories
contribute to the improvements in the diagram above.

A good example of computational improvements can be found in the
https://github.com/bitcoin-core/secp256k1[libsecp256k1] library, which
among other things, implements the cryptographic primitives needed to
make and verify digital signatures. Pieter Wuille is one of the
contributors to this library, and he
https://twitter.com/pwuille/status/1450471673321381896[wrote a twitter
thread] showcasing the performance improvements made by various pull
requests.

.Performance of signature verification over time, with significant pull requests marked on the timeline. Source: https://twitter.com/pwuille/status/1450471673321381896
image::libsecp256k1speedups.png[{half-width}]

The graph shows the trend for two different 64-bit CPU types, ARM and x86.
The difference in performance is due to the more specialized instructions
available on x86 compared to the ARM architecture, which has fewer,
more generic instructions. But the general trend is the same for both
architectures. Note that the Y-axis is logarithmic, which makes the
improvements look less impressive than they actually are.

There are also several good examples of space savings contributing to
performance improvements. In a
https://murchandamus.medium.com/2-of-3-multisig-inputs-using-pay-to-taproot-d5faf2312ba3[Medium
blog post] about Taproot's contribution to space savings, user Murch
compared how much block space a 2-of-3 threshold signature would
require, both without using Taproot and using Taproot in various ways.

.Space savings for different spending types Taproot and legacy versions.
image::murch-taproot.png[{half-width}]

A 2-of-3 multisig using native segwit would require a total of
104.5+43 vB = 147.5 vB, while the most space conservative Taproot
usage would in the standard use case require only 57.5+43 vB = 100.5
vB. At worst, in rare cases, like when a standard signer is
not available for some reason, 107.5+43 vB = 150.5 vB. You don't have
to understand all the details, but it should give you an idea
of how developers think about space savings. Every little byte counts.

Apart from the inward scaling going on in Bitcoin software, there are
also some ways that users can contribute to inward scaling. They can
make their transactions in more intelligent ways to save on
transaction fees while simultaneously decreasing their footprints on
full node requirements. Two commonly used techniques are called
transaction batching and output consolidation.

The idea with transaction batching is to combine multiple payments
into one single transaction, instead of using one transaction per
payment. This can save you a lot of fees, and at the same time, reduce
the block space load.

.Transaction batching combines multiple payments into a single transaction to save on fees.
image::tx-batching.png[{big-width}]

Output consolidation means that you take advantage of periods of low
block space demand to combine your outputs into a single output. This
can reduce your fee cost later, when you need to make a payment during
high block space demand.

.Output consolidation. Melt your coins into one big coin when fees are low to save fees later.
image::utxo-consolidation.png[{big-width}]

It may not be obvious how output consolidation contributes to inward
scaling. After all, the total amount of blockchain data even slightly
increases with this method, but the UTXO set, the database that keeps
track of who owns which coins, decreases because you spend more UTXOs
than you create. This alleviates the burden for full nodes to maintain
their UTXO sets.

Unfortunately, however, these two techniques of _UTXO management_ could
be bad for your own or your payees' privacy. In the batching case, a
payee will know that all these outputs are from you to other payees
(except possibly the change). In the UTXO consolidation case, you
reveal that the outputs you consolidate belong to the same wallet. So
you may have to make a trade-off between cost efficiency and privacy.

==== Layered scaling

The most impactful approach to scaling is probably layering. The
general idea of layering is that a protocol can settle payments
between users without adding transactions to the blockchain.
This was already discussed briefly in <<trustlessness>> and
<<privacymeasures>>.

A layered protocol begins with two or more people agreeing
on a start transaction that's put on the blockchain, as illustrated in
<<fig-scaling-layer>>.

[[fig-scaling-layer]]
.A typical layer 2 protocol on top of Bitcoin, layer 1.
image::scaling-layer.png[]

How this start transaction is created varies between protocols, but a
common theme is that the participants create an unsigned start
transaction. Then they create a number of pre-signed punishment
transactions that spend the output of the start transaction in various
ways. Then the start transaction is fully signed and published to the
blockchain. The punishment transactions can then be fully signed and
published to the blockchain to punish a misbehaving party. This
incentivizes the participants to keep their promises so that the
protocol can work in a trustless way.

After the start transaction is on the blockchain, the protocol can do
what it's supposed to do, for example, super-fast payment between
participants, or some privacy enhancing techniques, or to do more
advanced scripting not supported on Bitcoin's blockchain.

We won't detail how specific protocols work, but as
you can see in <<fig-scaling-layer>>, the blockchain is rarely used
during the protocol's life cycle. All the juicy action happens
_off-chain_. We've seen how this can be a win for <<privacy,privacy>>
if done right, but it can also be a big win for scalability.

In a https://www.reddit.com/r/Bitcoin/comments/438hx0/a_trip_to_the_moon_requires_a_rocket_with/[Reddit post] titled "`A trip to the moon requires a rocket with
multiple stages or otherwise the rocket equation will eat your
lunch... packing everyone in clown-car style into a trebuchet and
hoping for success is right out.`", Gregory Maxwell explains how
layering is our best shot at getting Bitcoin to scale by orders of
magnitudes.

He starts by emphasizing the fallacy in viewing Visa or Mastercard as
Bitcoin's main competitors and how increasing the maximum block size
is a bad approach to meet said competition. Then he's talking about
how to make some real difference using layers.

[quote, Gregory Maxwell, r/Bitcoin on Reddit]
____
So-- Does that mean that Bitcoin can't be a big winner as a payments
technology? No. But to reach the kind of capacity required to serve
the payments needs of the world we must work more intelligently.

From its very beginning Bitcoin was design to incorporate layers in
secure ways through its smart contracting capability (What, do you
think that was just put there so people could wax-philosophic about
meaningless "DAOs"?). In effect we will use the Bitcoin system as a
highly accessible and perfectly trustworthy robotic judge and conduct
most of our business outside of the court room-- but transact in such
a way that if something goes wrong we have all the evidence and
established agreements so we can be confident that the robotic court
will make it right. (Geek sidebar: If this seems impossible, go read
this old post on transaction cut-through)

This is possible precisely because of the core properties of
Bitcoin. A censorable or reversible base system is not very suitable
to build powerful upper layer transaction processing on top of... and
if the underlying asset isn't sound, there is little point in
transacting with it at all.
____

The analogy with the judge is quite illustrative of how layering
works. This judge must be incorruptible, and never change her
mind; otherwise, the layers above Bitcoin's base layer will not work
reliably.

He later makes a point about centralized services. There's usually no
problem with trusting a central server with trivial amounts of Bitcoin
to get things done. That's also layered scaling.

Many years have passed since Maxwell wrote the piece above, and his
words still stand correct. The success of the Lightning Network proves
that layering is indeed a way forward to increase the utility of
Bitcoin.
